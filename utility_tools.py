# utility_tools.py

import os
import torch
from typing import List, Optional

from langchain_core.documents import Document
from langchain_core.tools import tool
from langchain_core.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from pydantic import BaseModel, Field

from transformers import AutoTokenizer, AutoModelForSequenceClassification

import config

# =========================================================
# ê³µí†µ ìœ í‹¸
# =========================================================

def format_docs(docs: List[Document], max_chars: int = 15000) -> str:
    """
    LangChain Document ë¦¬ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ì— ë„£ê¸° ì‰¬ìš´ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©.
    """
    contents = [d.page_content.strip() for d in docs if isinstance(d, Document) and d.page_content]
    if not contents:
        return "[NO CONTENT]"
    joined = "\n\n---\n\n".join(contents)
    return (joined[:max_chars] + "\n\n...[ë‚´ìš© ì¼ë¶€ ìƒëµ]...") if len(joined) > max_chars else joined


# =========================================================
# Reranker ì „ì—­ ë¡œë“œ (ì˜µì…˜)
# =========================================================

_USE_RERANKER: bool = bool(getattr(config, "USE_RERANKER", False))
_RERANKER_MODEL_NAME: str = getattr(
    config,
    "RERANKER_MODEL_NAME",
    getattr(config, "RERANKER_MODEL", "BAAI/bge-reranker-v2-m3")
)

_rerank_tokenizer = None
_rerank_model = None

if _USE_RERANKER:
    try:
        print(f"[utility_tools] ë¦¬ë­ì»¤ ë¡œë“œ: {_RERANKER_MODEL_NAME}")
        _rerank_tokenizer = AutoTokenizer.from_pretrained(_RERANKER_MODEL_NAME)
        _rerank_model = AutoModelForSequenceClassification.from_pretrained(_RERANKER_MODEL_NAME)
        print("[utility_tools] ë¦¬ë­ì»¤ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"[utility_tools] ë¦¬ë­ì»¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        _USE_RERANKER = False
        _rerank_tokenizer = None
        _rerank_model = None


# =========================================================
# Vector Store (FAISS) ë¡œë“œ
# =========================================================

def _load_faiss() -> FAISS:
    emb = OpenAIEmbeddings(
        model=getattr(config, "OPENAI_EMBEDDING_MODEL", "text-embedding-3-large"),
        dimensions=getattr(config, "OPENAI_EMBEDDING_DIMENSIONS", None),
    )
    vs_path = getattr(config, "VECTOR_STORE_PATH", getattr(config, "VECTOR_DB_PATH", "vector_store"))
    return FAISS.load_local(vs_path, emb, allow_dangerous_deserialization=True)


def _dedup(docs: List[Document]) -> List[Document]:
    seen = set()
    out: List[Document] = []
    for d in docs:
        key = (d.metadata.get("source"), d.metadata.get("page"), hash(d.page_content))
        if key in seen:
            continue
        seen.add(key)
        out.append(d)
    return out


def _rerank(query: str, docs: List[Document], out_k: int) -> List[Document]:
    if not (_USE_RERANKER and _rerank_model and _rerank_tokenizer and docs):
        return docs[:out_k]
    pairs = [(query, d.page_content) for d in docs]
    inputs = _rerank_tokenizer(pairs, padding=True, truncation=True, return_tensors="pt", max_length=512)
    with torch.no_grad():
        scores = _rerank_model(**inputs).logits.squeeze().tolist()
    ranked = [doc for _, doc in sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)]
    return ranked[:out_k]


# =========================================================
# Tool 1: Vector Store RAG ê²€ìƒ‰
#  - OpenAI ì„ë² ë”©(ë‹¤êµ­ì–´) ì‚¬ìš©
#  - (ì˜µì…˜) dual_queries ë³‘í–‰ ê²€ìƒ‰
# =========================================================

@tool
def vector_store_rag_search(
    query: str,
    top_k: int = None,
    rerank_k: int = None
) -> List[Document]:
    """
    ë¡œì»¬ FAISS ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ìœ ì‚¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    - query: ê¸°ë³¸(ì£¼ë¡œ ì˜ì–´) ì¿¼ë¦¬
    - top_k: 1ì°¨ í›„ë³´ ê°œìˆ˜(ë¯¸ì§€ì • ì‹œ config.TOP_K_PER_QUERY*2 ì •ë„ë¡œ ë‚´ë¶€ ì¡°ì •)
    - rerank_k: ìµœì¢… ë°˜í™˜ ê°œìˆ˜(ë¯¸ì§€ì • ì‹œ config.TOP_K_PER_QUERY)
    """
    print(f"ğŸ› ï¸ RAG Tool ì‹¤í–‰: '{query}'")
    if not query:
        return []

    try:
        vs = _load_faiss()

        cfg_topk = int(getattr(config, "TOP_K_PER_QUERY", 5))
        out_k = int(rerank_k or cfg_topk)
        fetch_k = int(top_k or max(out_k * 2, cfg_topk * 2))

        retriever = vs.as_retriever(search_kwargs={"k": fetch_k})
        candidates: List[Document] = retriever.invoke(query)

        # ì¤‘ë³µ ì œê±° í›„ (ì˜µì…˜) ë¦¬ë­í‚¹
        uniq = _dedup(candidates)
        ranked = _rerank(query, uniq, out_k=out_k)

        return ranked[:out_k]

    except Exception as e:
        print(f"âŒ RAG Tool ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return []


# =========================================================
# Tool 2: (LLM ê¸°ë°˜) ì‹¬ì¸µ ì›¹ ë¦¬ì„œì¹˜
# =========================================================

class SearchResult(BaseModel):
    title: str = Field(description="The title of the search result.")
    url: str = Field(description="The URL of the search result.")
    summary: str = Field(description="A detailed, objective summary addressing the user's query.")

class SearchResults(BaseModel):
    results: List[SearchResult]

@tool
def deep_research_web_search(query: str, max_results: int = 3) -> List[Document]:
    """
    gpt-4.1 ë“± LLMì„ ì´ìš©í•´ êµ¬ì¡°í™”ëœ ì›¹ ë¦¬ì„œì¹˜ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    (ì‹¤ì œ ë„¤ë¹„ê²Œì´ì…˜/ë¸Œë¼ìš°ì§• ì—†ì´ ìš”ì•½ ê¸°ë°˜)
    """
    print(f"ğŸ› ï¸ Deep Research Tool ì‹¤í–‰: '{query}'")
    if not query:
        return []

    try:
        llm = ChatOpenAI(model=getattr(config, "LLM_MODEL_WEB", "gpt-4.1"), temperature=0)
        structured_llm = llm.with_structured_output(SearchResults)

        prompt = PromptTemplate.from_template(
"""You are an expert, objective web researcher working specifically on Samsung-related questions.
Assume that most incoming queries are about Samsung products, services, specifications, policies, or company news.

Goal:
Provide {max_results} distinct, non-overlapping results that directly address the userâ€™s query,
prioritizing Samsung-owned official sources while allowing reputable third-party sources only if official coverage is insufficient.

Ranking & Source Policy:
- Rank Samsung official domains first (samsung.com, semiconductor.samsung.com, news.samsung.com, images.samsung.com, developer.samsung.com, research.samsung.com).
- If official coverage is insufficient, you MAY include reputable third-party sources (e.g., major tech media, standards bodies, vendor whitepapers),
  but keep the summaries factual and clearly avoid speculation.

Requirements:
- Each result must include:
  (1) a clear, concise title,
  (2) a comprehensive, factual summary in Korean (useful on its own, no marketing fluff; if third-party, reflect that neutrally),
  (3) the canonical, direct https URL.
- Prefer diverse domains/perspectives; avoid near-duplicates.
- Do NOT fabricate facts or URLs. If a crucial detail is unknown, explicitly state it in the summary.
- Be precise and concrete: include key numbers, model names, SKUs, standards, or dates when available.

Query: '{query}'

Return ONLY a structured object that matches the given schema (no extra text).
"""
        )

        chain = prompt | structured_llm
        response: SearchResults = chain.invoke({"query": query, "max_results": max_results})

        docs: List[Document] = []
        if response and response.results:
            for r in response.results:
                docs.append(
                    Document(
                        page_content=f"Title: {r.title}\nSummary: {r.summary}",
                        metadata={"source": r.url, "title": r.title}
                    )
                )
        return docs

    except Exception as e:
        print(f"âŒ Deep Research Tool ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return []
