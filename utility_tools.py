# utility_tools.py

import os
import requests
import torch
from typing import List

from langchain_core.documents import Document
from langchain_core.tools import tool
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForSequenceClassification

import config

# --- Reranker ëª¨ë¸ ì „ì—­ ë¡œë“œ ---
print("ReRanker ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
try:
    rerank_tokenizer = AutoTokenizer.from_pretrained(config.RERANKER_MODEL)
    rerank_model = AutoModelForSequenceClassification.from_pretrained(config.RERANKER_MODEL)
    print("ReRanker ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
except Exception as e:
    print(f"ReRanker ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    rerank_model = None

def format_docs(docs: List[Document], max_chars: int = 15000) -> str:
    """
    LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ LLM í”„ë¡¬í”„íŠ¸ì— ì‚½ì…í•˜ê¸° ì¢‹ì€ ë‹¨ì¼ ë¬¸ìì—´ë¡œ ê²°í•©í•©ë‹ˆë‹¤.
    """
    contents = [doc.page_content.strip() for doc in docs if isinstance(doc, Document) and doc.page_content]
    if not contents:
        return "[NO CONTENT]"

    joined_content = "\n\n---\n\n".join(contents)
    return joined_content[:max_chars] + "\n\n...[ë‚´ìš© ì¼ë¶€ ìƒëµ]..." if len(joined_content) > max_chars else joined_content

# --- Tool 1: Vector Store RAG ê²€ìƒ‰ ---
@tool
def vector_store_rag_search(query: str, top_k: int = 10, rerank_k: int = 3) -> List[Document]:
    """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸(query)ì„ ë°›ì•„ ë¡œì»¬ Vector Store(FAISS)ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ ,
    ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ë¦¬ë­í‚¹(reranking)ì„ ìˆ˜í–‰í•œ í›„ ìƒìœ„ ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë‚´ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ì°¾ì•„ì•¼ í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ› ï¸ RAG Tool ì‹¤í–‰: '{query}'")
    if not query: return []
    try:
        embedding = HuggingFaceEmbeddings(model_name=config.EMBEDDING_MODEL)
        db = FAISS.load_local(config.VECTOR_STORE_PATH, embedding, allow_dangerous_deserialization=True)
        candidate_docs = db.similarity_search(query, k=top_k)

        if rerank_model and candidate_docs:
            pairs = [(query, doc.page_content) for doc in candidate_docs]
            inputs = rerank_tokenizer(pairs, padding=True, truncation=True, return_tensors="pt", max_length=512)
            with torch.no_grad():
                scores = rerank_model(**inputs).logits.squeeze().tolist()
            scored_docs = sorted(zip(scores, candidate_docs), key=lambda x: x[0], reverse=True)
            return [doc for score, doc in scored_docs[:rerank_k]]
        return candidate_docs[:rerank_k]
    except Exception as e:
        print(f"âŒ RAG Tool ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

# --- Tool 2: SerpAPI ì›¹ ê²€ìƒ‰ ---
@tool
def serpapi_web_search(query: str, max_results: int = 5) -> List[Document]:
    """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸(query)ì„ ë°›ì•„ SerpAPIë¥¼ í†µí•´ ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ ,
    ê²€ìƒ‰ ê²°ê³¼ë¥¼ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ìµœì‹  ì •ë³´ë‚˜ ì™¸ë¶€ ì§€ì‹ì´ í•„ìš”í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ› ï¸ Web Search Tool ì‹¤í–‰: '{query}'")
    if not query: return []
    api_key = os.getenv("SERPAPI_API_KEY")
    if not api_key:
        print("âŒ SerpAPI í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."); return []

    endpoint = "https://serpapi.com/search"
    params = {"engine": "google", "q": query, "api_key": api_key}
    try:
        response = requests.get(endpoint, params=params); response.raise_for_status()
        data, docs = response.json(), []
        for result_type in ["answer_box", "knowledge_graph", "organic_results"]:
            if result_type in data:
                results = data[result_type]
                if isinstance(results, dict) and result_type != "organic_results": results = [results]
                for item in results:
                    if len(docs) >= max_results: break
                    content = item.get("snippet") or item.get("answer") or item.get("description")
                    if content:
                        docs.append(Document(page_content=content.strip(), metadata={"source": item.get("link", result_type), "title": item.get("title", "")}))
        return docs
    except Exception as e:
        print(f"âŒ Web Search Tool ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []