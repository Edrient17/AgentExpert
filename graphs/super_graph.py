# graphs/super_graph.py

from typing import Literal, Optional
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_openai import ChatOpenAI
from langchain_core.messages import ToolMessage, HumanMessage
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END

import config
from state import AgentState

# --- ë§¤ë‹ˆì € ì—ì´ì „íŠ¸ì˜ ê²°ì •ì„ ìœ„í•œ Pydantic ìŠ¤í‚¤ë§ˆ ---
class ManagerDecision(BaseModel):
    """ë§¤ë‹ˆì €ì˜ ê²°ì • ìŠ¤í‚¤ë§ˆ"""
    next_team: Literal["team1", "team2", "team3", "end"] = Field(description="ë‹¤ìŒì— ì‘ì—…ì„ ìˆ˜í–‰í•  íŒ€ ë˜ëŠ” ì›Œí¬í”Œë¡œìš° ì¢…ë£Œ ì—¬ë¶€")
    feedback: Optional[str] = Field(description="ì‘ì—…ì„ ìˆ˜ì •í•´ì•¼ í•  ê²½ìš°, í•´ë‹¹ íŒ€ì—ê²Œ ì „ë‹¬í•  êµ¬ì²´ì ì¸ í•œê¸€ í”¼ë“œë°±")
    reason: str = Field(description="ê²°ì •ì— ëŒ€í•œ ê°„ë‹¨í•œ í•œê¸€ ìš”ì•½")

# --- ìŠˆí¼ê·¸ë˜í”„ì˜ ë…¸ë“œë“¤ ---

def manager_agent(state: AgentState) -> dict:

    print("--- MANAGER: ì‘ì—… ê²€í†  ë° ë‹¤ìŒ ë‹¨ê³„ ê²°ì • ---")

    global_loop_count = state.get("global_loop_count", 0)
    last_message = state['messages'][-1]

    last_name = getattr(last_message, 'name', 'N/A')
    last_content = getattr(last_message, 'content', '')
    if last_name == "team2_evaluator" and last_content == "fail":
        print("âš ï¸ ë§¤ë‹ˆì €: Team2 ì‹¤íŒ¨ ê°ì§€ â†’ Team1ë¡œ ë˜ëŒë¦¼(ê²°ì •ì  ê°€ë“œ).")
        global_loop_count += 1
        next_team = "team1"
        feedback = "Team2ê°€ ìë£Œ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì´ê³  í˜‘ì˜ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”."
        # ê¸€ë¡œë²Œ ë£¨í”„ ì œí•œ ì²´í¬
        if global_loop_count >= config.MAX_GLOBAL_LOOPS:
            print(f"âŒ ê¸€ë¡œë²Œ ë£¨í”„ ì œí•œ({config.MAX_GLOBAL_LOOPS}íšŒ) ì´ˆê³¼. ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return {
                "next_team_to_call": "end",
                "manager_feedback": "Process terminated to prevent an infinite loop.",
                "global_loop_count": global_loop_count,
            }
        # Team1 ì¬ì‹œì‘ ì‹œ ì¬ì‹œë„ ì¹´ìš´í„° ë¦¬ì…‹
        return {
            "next_team_to_call": next_team,
            "manager_feedback": feedback,
            "global_loop_count": global_loop_count,
            "team1_retries": 0,
        }

    user_question = next((msg.content for msg in state['messages'] if isinstance(msg, HumanMessage)), "")

    parser = JsonOutputParser(p_object=ManagerDecision)
    prompt = PromptTemplate.from_template("""
You are the project manager of a multi-agent RAG system. Your role is to review the work of your teams (Team1, Team2, Team3) and decide the next step with surgical precision.

**CONTEXT:**
- User's original question: "{user_question}"
- The last message from the previous team:
  - Team/Node: "{last_message_name}"
  - Content/Result: "{last_message_content}"

**YOUR TASK:**
Based on the context, decide the next team to call or to end the process. You can also provide feedback to a team to ask for revisions. The goal is to solve the problem by addressing the root cause.

**DECISION LOGIC (Follow these rules STRICTLY):**
1.  **If `{last_message_name}` is "team1_evaluator":**
    - If content is "pass": The query analysis is good. Call `team2` to start information retrieval.
    - If content is "fail": The analysis is poor. Call `team1` again with specific feedback to improve the queries.
2.  **If `{last_message_name}` is "team2_evaluator":**
    - If content is "pass": The retrieved documents are relevant. Call `team3` to generate the final answer.
    - If content is "fail": The documents are not good enough. This indicates the search queries were likely poor. **Call `team1` again.** Provide specific feedback, instructing them to generate better, more precise search queries based on the failure reason. For example: "The initial queries were too broad and did not yield relevant documents. Please generate more specific queries."
3.  **If `{last_message_name}` is "final_evaluator" (from Team 3):**
    - If content is "pass": The final answer is excellent. The job is done. Call `end`.
    - If content is "fail": The answer is not satisfactory. Call `team3` again with feedback to revise the answer.
4.  **If the last message indicates a critical tool failure:**
    - The process cannot continue. Call `end` and provide a reason.

**OUTPUT (JSON ONLY):**
Provide your decision in the following JSON format.
{schema}
""").partial(schema=parser.get_format_instructions())

    llm = ChatOpenAI(model=config.LLM_MODEL_SUPER_ROUTER, temperature=0.0, model_kwargs={"response_format": {"type": "json_object"}})
    chain = prompt | llm | parser

    try:
        result = chain.invoke({
            "user_question": user_question,
            "last_message_name": getattr(last_message, 'name', 'N/A'),
            "last_message_content": last_message.content
        })
        
        next_team = result.get("next_team", "end")
        reason = result.get("reason", "LLMìœ¼ë¡œë¶€í„° ì´ìœ ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        feedback = result.get("feedback")

        is_backward_loop = (getattr(last_message, 'name', 'N/A') == "team2_evaluator" and next_team == "team1")
        
        if is_backward_loop:
            global_loop_count += 1
            print(f"ğŸ”„ ë§¤ë‹ˆì €ê°€ ë°±ì›Œë“œ ë£¨í”„ë¥¼ ê°ì§€í–ˆìŠµë‹ˆë‹¤. ê¸€ë¡œë²Œ ë£¨í”„ ì¹´ìš´íŠ¸: {global_loop_count}")
            if global_loop_count >= config.MAX_GLOBAL_LOOPS:
                print(f"âŒ ê¸€ë¡œë²Œ ë£¨í”„ ì œí•œ({config.MAX_GLOBAL_LOOPS}íšŒ)ì„ ì´ˆê³¼í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                next_team = "end" # Override the decision and force termination
                feedback = "Process terminated to prevent an infinite loop."

        print(f"ğŸ§  ë§¤ë‹ˆì € ê²°ì •: {next_team}, ì´ìœ : {reason}")
        
        update_dict = {
            "next_team_to_call": next_team,
            "manager_feedback": feedback,
            "global_loop_count": global_loop_count,
        }

        # íŠ¹ì • íŒ€ìœ¼ë¡œ ì‘ì—…ì„ ë˜ëŒë ¤ ë³´ë‚¼ ë•Œ, í•´ë‹¹ íŒ€ì˜ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê¸°í™”
        if next_team == "team1":
            update_dict["team1_retries"] = 0
        elif next_team == "team2":
            update_dict["team2_retries"] = 0
        elif next_team == "team3":
            update_dict["team3_retries"] = 0
        
        return update_dict
    
    except Exception as e:
        print(f"âŒ ë§¤ë‹ˆì € ì—ì´ì „íŠ¸ ì˜¤ë¥˜: {e}")
        return {"next_team_to_call": "end", "manager_feedback": "ë§¤ë‹ˆì € ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}


def create_super_graph(team1_app, team2_app, team3_app):
    """
    ë§¤ë‹ˆì € ì—ì´ì „íŠ¸ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ëª¨ë“  íŒ€ ì„œë¸Œê·¸ë˜í”„ë¥¼ í†µí•©í•˜ëŠ” ìŠˆí¼ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    builder = StateGraph(AgentState)

    builder.add_node("team1", team1_app)
    builder.add_node("team2", team2_app)
    builder.add_node("team3", team3_app)
    builder.add_node("manager", manager_agent)

    builder.set_entry_point("team1")

    builder.add_edge("team1", "manager")
    builder.add_edge("team2", "manager")
    builder.add_edge("team3", "manager")

    def route_from_manager(state: AgentState) -> str:
        next_team = state.get("next_team_to_call")
        print(f"ğŸš¦ ìŠˆí¼ê·¸ë˜í”„ ë¼ìš°í„°: ë‹¤ìŒ ëª©ì ì§€ëŠ” '{next_team}'")
        if not next_team:
            return "end" # next_teamì´ ì—†ëŠ” ì˜ˆì™¸ì ì¸ ê²½ìš° ì¢…ë£Œ
        return next_team

    builder.add_conditional_edges(
        "manager",
        route_from_manager,
        {
            "team1": "team1",
            "team2": "team2",
            "team3": "team3",
            "end": END
        }
    )

    return builder.compile()