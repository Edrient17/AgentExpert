# graphs/super_graph.py

from typing import Literal, Optional
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_openai import ChatOpenAI
from langchain_core.messages import ToolMessage, HumanMessage
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END

import config
from state import AgentState

# --- ë§¤ë‹ˆì € ì—ì´ì „íŠ¸ì˜ ê²°ì •ì„ ìœ„í•œ Pydantic ìŠ¤í‚¤ë§ˆ ---
class ManagerDecision(BaseModel):
    """ë§¤ë‹ˆì €ì˜ ê²°ì • ìŠ¤í‚¤ë§ˆ"""
    next_team: Literal["team1", "team2", "team3", "end"] = Field(description="ë‹¤ìŒì— ì‘ì—…ì„ ìˆ˜í–‰í•  íŒ€ ë˜ëŠ” ì›Œí¬í”Œë¡œìš° ì¢…ë£Œ ì—¬ë¶€")
    feedback: Optional[str] = Field(description="ì‘ì—…ì„ ìˆ˜ì •í•´ì•¼ í•  ê²½ìš°, í•´ë‹¹ íŒ€ì—ê²Œ ì „ë‹¬í•  êµ¬ì²´ì ì¸ í•œê¸€ í”¼ë“œë°±")
    reason: str = Field(description="ê²°ì •ì— ëŒ€í•œ ê°„ë‹¨í•œ í•œê¸€ ìš”ì•½")

# --- ìŠˆí¼ê·¸ë˜í”„ì˜ ë…¸ë“œë“¤ ---

def manager_agent(state: AgentState) -> dict:

    print("--- MANAGER: ì‘ì—… ê²€í†  ë° ë‹¤ìŒ ë‹¨ê³„ ê²°ì • ---")

    global_loop_count = state.get("global_loop_count", 0)
    last_message = state['messages'][-1]

    last_name = getattr(last_message, 'name', 'N/A')
    last_content = getattr(last_message, 'content', '')

    user_question = next((msg.content for msg in state['messages'] if isinstance(msg, HumanMessage)), "")

    try:
        if last_name == "team1_evaluator" and str(last_content).strip() == "pass":
            is_simple = state.get("is_simple_query", "No")
            if is_simple == "Yes":
                print("ğŸ§­ Manager ë‹¨ì¶• ë¼ìš°íŒ…: ê°„ë‹¨ì§ˆë¬¸ â†’ Team3 ì§í–‰")
                return {
                    "next_team_to_call": "team3",
                    "manager_feedback": None,
                    "global_loop_count": global_loop_count,
                    "team3_retries": 0
                }
            else:
                print("ğŸ§­ Manager ë‹¨ì¶• ë¼ìš°íŒ…: ì¼ë°˜ì§ˆë¬¸ â†’ Team2 íƒìƒ‰")
                return {
                    "next_team_to_call": "team2",
                    "manager_feedback": None,
                    "global_loop_count": global_loop_count,
                    "team2_retries": 0
                }
    except Exception as e:
        print(f"âš ï¸ is_simple_query ê¸°ë°˜ ë¼ìš°íŒ… ì‹¤íŒ¨: {e} (ê¸°ë³¸ LLM ë¼ìš°íŒ…ìœ¼ë¡œ ì§„í–‰)")

    parser = JsonOutputParser(p_object=ManagerDecision)
    prompt = PromptTemplate.from_template("""
You are the project manager of a multi-agent RAG system. Your role is to review the work of your teams (Team1, Team2, Team3) and decide the next step with surgical precision.

**CONTEXT:**
- User's original question: "{user_question}"
- The last message from the previous team:
  - Team/Node: "{last_message_name}"
  - Content/Result: "{last_message_content}"

**YOUR TASK:**
Based on the context, decide the next team to call or to end the process. You can also provide feedback to a team to ask for revisions. The goal is to solve the problem by addressing the root cause.

**DECISION LOGIC (Follow these rules STRICTLY):**
Your decision MUST be based on the `last_message_name` and its `content`.

- **WHEN `last_message_name` is "team1_evaluator":**
  - If `content` is "pass", call `team2`.
  - If `content` is "fail", call `team1` with feedback.

- **WHEN `last_message_name` is "team2_evaluator":**
  - If `content` is "pass", call `team3`.
  - If `content` is "fail", call `team1` with feedback to fix the search queries. This is a critical step; poor documents usually mean poor queries.

- **WHEN `last_message_name` is "team3_evaluator":**
  - If `content` is "pass", the entire process is complete. Call `end`.
  - If `content` is "fail", call `team3` again with feedback for revision.

- **IF a tool failure is reported in the last message:**
  - The process cannot continue. Call `end` and provide a reason.

**OUTPUT (JSON ONLY):**
Provide your decision in the following JSON format.
{schema}
""").partial(schema=parser.get_format_instructions())

    llm = ChatOpenAI(model=config.LLM_MODEL_SUPER_ROUTER, temperature=0.0, model_kwargs={"response_format": {"type": "json_object"}})
    chain = prompt | llm | parser

    try:
        result = chain.invoke({
            "user_question": user_question,
            "last_message_name": last_name,
            "last_message_content": last_content
        })
        
        next_team = result.get("next_team", "end")
        reason = result.get("reason", "LLMìœ¼ë¡œë¶€í„° ì´ìœ ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        feedback = result.get("feedback")

        is_backward_loop = (getattr(last_message, 'name', 'N/A') == "team2_evaluator" and next_team == "team1")
        
        if is_backward_loop:
            global_loop_count += 1
            print(f"ğŸ”„ ë§¤ë‹ˆì €ê°€ ë°±ì›Œë“œ ë£¨í”„ë¥¼ ê°ì§€í–ˆìŠµë‹ˆë‹¤. ê¸€ë¡œë²Œ ë£¨í”„ ì¹´ìš´íŠ¸: {global_loop_count}")
            if global_loop_count >= config.MAX_GLOBAL_LOOPS:
                print(f"âŒ ê¸€ë¡œë²Œ ë£¨í”„ ì œí•œ({config.MAX_GLOBAL_LOOPS}íšŒ)ì„ ì´ˆê³¼í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                next_team = "end" # Override the decision and force termination
                feedback = "Process terminated to prevent an infinite loop."

        print(f"ğŸ§  ë§¤ë‹ˆì € ê²°ì •: {next_team}, ì´ìœ : {reason}")
        
        update_dict = {
            "next_team_to_call": next_team,
            "manager_feedback": feedback,
            "global_loop_count": global_loop_count,
        }

        # íŠ¹ì • íŒ€ìœ¼ë¡œ ì‘ì—…ì„ ë˜ëŒë ¤ ë³´ë‚¼ ë•Œ, í•´ë‹¹ íŒ€ì˜ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê¸°í™”
        if next_team == "team1":
            update_dict["team1_retries"] = 0
        elif next_team == "team2":
            update_dict["team2_retries"] = 0
        elif next_team == "team3":
            update_dict["team3_retries"] = 0
        
        return update_dict
    
    except Exception as e:
        print(f"âŒ ë§¤ë‹ˆì € ì—ì´ì „íŠ¸ ì˜¤ë¥˜: {e}")
        return {"next_team_to_call": "end", "manager_feedback": "ë§¤ë‹ˆì € ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}


def create_super_graph(team1_app, team2_app, team3_app):
    """
    ë§¤ë‹ˆì € ì—ì´ì „íŠ¸ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ëª¨ë“  íŒ€ ì„œë¸Œê·¸ë˜í”„ë¥¼ í†µí•©í•˜ëŠ” ìŠˆí¼ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    builder = StateGraph(AgentState)

    builder.add_node("team1", team1_app)
    builder.add_node("team2", team2_app)
    builder.add_node("team3", team3_app)
    builder.add_node("manager", manager_agent)

    builder.set_entry_point("team1")

    builder.add_edge("team1", "manager")
    builder.add_edge("team2", "manager")
    builder.add_edge("team3", "manager")

    def route_from_manager(state: AgentState) -> str:
        next_team = state.get("next_team_to_call")
        print(f"ğŸš¦ ìŠˆí¼ê·¸ë˜í”„ ë¼ìš°í„°: ë‹¤ìŒ ëª©ì ì§€ëŠ” '{next_team}'")
        if not next_team:
            return "end" # next_teamì´ ì—†ëŠ” ì˜ˆì™¸ì ì¸ ê²½ìš° ì¢…ë£Œ
        return next_team

    builder.add_conditional_edges(
        "manager",
        route_from_manager,
        {
            "team1": "team1",
            "team2": "team2",
            "team3": "team3",
            "end": END
        }
    )

    return builder.compile()