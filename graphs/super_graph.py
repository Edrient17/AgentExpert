# graphs/super_graph.py

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END

import config
from state import GlobalState

def retrieval_necessity_router(state: GlobalState) -> str:
    """
    Team 1 ì™„ë£Œ í›„, LLMì„ ì´ìš©í•´ Team 2(ì •ë³´ ê²€ìƒ‰)ê°€ í•„ìš”í•œì§€ íŒë‹¨í•˜ëŠ” **ë¼ìš°íŒ… í•¨ìˆ˜**ì…ë‹ˆë‹¤.
    """
    print("--- ROUTER: ì •ë³´ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ íŒë‹¨ ---")
    
    if state.get("status", {}).get("team1") == "fail":
        print("ğŸš¦ ë¼ìš°í„°: Team 1 ì‹¤íŒ¨ ê°ì§€. ì›Œí¬í”Œë¡œìš°ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return END

    question = state.get("q_en_transformed", "")
    
    prompt = PromptTemplate.from_template("""
You are a meticulous and safety-conscious router in a Q&A pipeline. Your critical task is to determine if a user's question can be answered *reliably* with your general knowledge, or if it requires consulting specific, external information to ensure accuracy and currency.

Your response must be a single word: either 'retrieve' or 'skip'.

Decision Criteria:

1.  retrieve: If the question requires external documents/knowledge (e.g., "Tell me about LangGraph"). 
2.  skip:  If the question does NOT require external knowledge (e.g., "Hi? What's your name?", "What is 2+2?"). 

When in doubt, always choose 'retrieve'.

Question: "{question}"
""").partial(question=question)

    llm = ChatOpenAI(model=config.LLM_MODEL_SUPER_ROUTER, temperature=0.0)
    chain = prompt | llm | StrOutputParser()
    
    try:
        decision = chain.invoke({})
        print(f"ğŸ§  ë¼ìš°í„° LLM ê²°ì •: '{decision}'")
        if "retrieve" in decision.lower():
            print("ğŸš¦ ë¼ìš°í„°: ì •ë³´ ê²€ìƒ‰ í•„ìš”. Team 2ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
            return "team2"
        else:
            print("ğŸš¦ ë¼ìš°í„°: ì •ë³´ ê²€ìƒ‰ ë¶ˆí•„ìš”. Team 2ë¥¼ ê±´ë„ˆë›°ê³  Team 3ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
            state["status"]["team2"] = "pass"
            return "team3"
    except Exception as e:
        print(f"âŒ ë¼ìš°í„° LLM ì‹¤í–‰ ì˜¤ë¥˜: {e}. ì•ˆì „í•˜ê²Œ Team 2ë¡œ ë³´ëƒ…ë‹ˆë‹¤.")
        return "team2"

def create_super_graph(team1_app, team2_app, team3_app):
    """
    ì§€ëŠ¥í˜• ë¼ìš°í„°ë¥¼ í¬í•¨í•˜ì—¬ ëª¨ë“  íŒ€ ì„œë¸Œê·¸ë˜í”„ë¥¼ í†µí•©í•©ë‹ˆë‹¤.
    """
    builder = StateGraph(GlobalState)

    # 1. ê° íŒ€ ì„œë¸Œê·¸ë˜í”„ë¥¼ ë…¸ë“œë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
    builder.add_node("team1", team1_app)
    builder.add_node("team2", team2_app)
    builder.add_node("team3", team3_app)

    # 2. ì—£ì§€ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.
    builder.set_entry_point("team1")
    
    builder.add_conditional_edges(
        "team1", # ì‹œì‘ ë…¸ë“œ
        retrieval_necessity_router, # íŒë‹¨ í•¨ìˆ˜
        { # íŒë‹¨ ê²°ê³¼ì— ë”°ë¥¸ ë¶„ê¸°
            "team2": "team2",
            "team3": "team3",
            END: END
        }
    )
    
    builder.add_edge("team2", "team3")
    builder.add_edge("team3", END)

    # 3. ìµœì¢… ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•©ë‹ˆë‹¤.
    return builder.compile()
