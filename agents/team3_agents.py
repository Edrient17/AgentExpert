# agents/team3_agents.py
import json
import uuid
from typing import Dict, Any

from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.messages import AIMessage, ToolMessage
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

import config
from state import AgentState
from utility_tools import format_docs, create_table_image

DOCS_ANSWER_PROMPT = PromptTemplate.from_template("""
You are the Team 3 answer generator in a Multi-Agent Q&A pipeline.
Your primary task is to synthesize an answer based on the provided passages.

You will receive a user query and the following passages.
                                                  
Passage Sources
1) RAG (Primary): The following is an internal/trust document.
2) Web (Secondary): Below is an external web search summary.

Decision rules:
- If there is a conflict between the RAG and the Web content, the RAG takes precedence.
- If the RAG is enough, the Web document will not be cited.                                               

Your job:
- Produce the final answer strictly in the requested output format and language.

<think>
Passages:
[RAG Passages]
{rag_context}

[Web Passages]
{web_context}

I will use both the information from these passages and my own prior knowledge to answer the question.
First, I will carefully examine whether the passages contain any misinformation, contradictions, or irrelevant details.
Then, I will combine verified facts from the passages with only safe, generally accepted prior knowledge to derive the correct answer.

Before finalizing the answer, I will double-check that every single statement in my generated answer is directly supported by the provided passages. If I cannot find direct evidence for a piece of information, I will remove it from the answer.

If conflicts remain or the evidence is insufficient after this check, I will output the no-information message instead of guessing.
I will NOT reveal any reasoning or the content of this <think> block in the final output.
</think>

{feedback_instructions}
                                                  
Requested format: {out_type}
Requested language: {answer_language}

Format guidelines (flexible within type):
- qa:
  â€¢ Begin with a one-sentence direct answer.
  â€¢ Then provide ample explanation (multiple short paragraphs and/or 3â€“8 bullets).
  â€¢ If a procedure is involved, include a numbered step list. May add "Edge cases", "Tips" sections if useful.
- bulleted:
  â€¢ 8â€“15 bullets with meaningful substance (â‰¤40 words each).
  â€¢ You may group bullets by mini-headings (bold) and use one level of sub-bullets when needed.
- table:
  â€¢ Produce a Markdown table with a header row; derive 3â€“9 sensible columns from the question/context.
  â€¢ Include as many rows as needed (up to ~100). Use "N/A" for missing values.
  â€¢ IMPORTANT: You MUST return ONLY the raw Markdown table content, starting with a header row (e.g., `| Header 1 | ... |`) and nothing else. Do NOT include any introductory or concluding sentences, code fences, or notes.
- report:
  â€¢ Design your own section plan (3â€“10 H2 sections) tailored to the question.
  â€¢ Each section 3â€“8 sentences; include lists/tables where helpful. Subsections (H3) allowed.

Grounding & safety rules:
- Use the passages as primary evidence. You MAY use prior knowledge only if it does not contradict the passages.
- If the passages are insufficient or conflicting, respond with the no-information message in the requested language and format:
  - ko: "ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
  - en: "The documents do not contain that information."
- Neither RAG Passages nor WEB Passages always exists.
- If any part of the answer relies *exclusively* on information from the 'Web Passages' (i.e., the information was not in the 'RAG Passages'), you must naturally indicate that this specific information is from a web source.
- Do NOT add any prefixes about requested format prior to the answer.
- Do NOT invent or hallucinate facts.
- Do NOT include external URLs or formal academic-style citations (e.g., [1], (Author, 2024)).
- DO NOT reveal the content of <think> or any reasoning steps.

Write STRICTLY in: {answer_language}

Inputs:
[user query]
{q_en_transformed}
                                      
Answer:
""")

GENERAL_ANSWER_PROMPT = PromptTemplate.from_template("""
You are a helpful AI assistant. Your task is to answer the user's question.

Your job:
- Produce the final answer strictly in the requested output format and language.

{feedback_instructions}

Requested format: {out_type}
Requested language: {answer_language}

Grounding & safety rules:
- Do NOT add any prefixes about requested format prior to the answer.
- At the end of the answer, indicate 'This answer is generated by LLM's own knowledge' as requested language.
                                                                                                                                                         
Format guidelines (flexible within type):
- qa:
  â€¢ Begin with a one-sentence direct answer.
  â€¢ Then provide ample explanation (multiple short paragraphs and/or 3â€“8 bullets).
  â€¢ If a procedure is involved, include a numbered step list. May add "Edge cases", "Tips" sections if useful.
- bulleted:
  â€¢ 8â€“15 bullets with meaningful substance (â‰¤40 words each).
  â€¢ You may group bullets by mini-headings (bold) and use one level of sub-bullets when needed.
- table:
  â€¢ Produce a Markdown table with a header row; derive 3â€“9 sensible columns from the question/context.
  â€¢ Include as many rows as needed (up to ~100). Use "N/A" for missing values.
  â€¢ IMPORTANT: You MUST return ONLY the raw Markdown table content, starting with a header row (e.g., `| Header 1 | ... |`) and nothing else. Do NOT include any introductory or concluding sentences, code fences, or notes.
- report:
  â€¢ Design your own section plan (3â€“10 H2 sections) tailored to the question.
  â€¢ Each section 3â€“8 sentences; include lists/tables where helpful. Subsections (H3) allowed.

Write STRICTLY in: {answer_language}

Inputs:
[Refined question]
{q_en_transformed}

Answer:
""")
# --- Pydantic ìŠ¤í‚¤ë§ˆ ---
class AnswerEvaluationResult(BaseModel):
    rules_compliance: bool
    question_coverage: float
    hallucination_score: float
    error_message: str = ""

def _get_context_from_history(state: AgentState) -> dict:
    """ë‹µë³€ ìƒì„±ì— í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. (rag ìš°ì„ )"""
    context = {
        "q_en_transformed": "",
        "output_format": ["qa", "ko"],
        "rag_docs": [],
        "web_docs": [],
        "docs": []
    }

    # 1) ìƒíƒœì— ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©(rag ìš°ì„  ê²°í•©)
    rag_from_state = state.get("rag_docs", [])
    web_from_state = state.get("web_docs", [])
    if rag_from_state or web_from_state:
        context["rag_docs"] = rag_from_state
        context["web_docs"] = web_from_state
        context["docs"] = rag_from_state + web_from_state  # rag ë¨¼ì €

    # 2) ë©”ì‹œì§€ì—ì„œ Team2 passì˜ ì¶”ê°€ì •ë³´ë¥¼ ì¡°íšŒ (ë°±ì—… ê²½ë¡œ)
    if not context["docs"]:
        for msg in reversed(state['messages']):
            if isinstance(msg, ToolMessage) and msg.name == "team2_evaluator" and msg.content == "pass":
                rag_docs = msg.additional_kwargs.get("rag_docs", [])
                web_docs = msg.additional_kwargs.get("web_docs", [])
                if rag_docs or web_docs:
                    context["rag_docs"] = rag_docs
                    context["web_docs"] = web_docs
                    context["docs"] = rag_docs + web_docs
                else:
                    # êµ¬ë²„ì „ í˜¸í™˜: í•©ë³¸ë§Œ ì˜¨ ê²½ìš°
                    context["docs"] = msg.additional_kwargs.get("retrieved_docs", [])
                break

    # 3) Team1ì˜ ì§ˆë¬¸/í¬ë§· ì •ë³´
    for msg in reversed(state['messages']):
        if isinstance(msg, ToolMessage) and msg.name == "team1_evaluator":
            context["q_en_transformed"] = msg.additional_kwargs.get("q_en_transformed", "")
            context["output_format"] = msg.additional_kwargs.get("output_format", ["qa", "ko"])
            break

    return context

# --- Node 1: ë‹µë³€ ìƒì„± (Worker) ---
def generate_answer(state: AgentState) -> Dict[str, Any]:
    print("--- AGENT: Team 3 (ë‹µë³€ ìƒì„±) ì‹¤í–‰ ---")

    manager_feedback = state.get("manager_feedback")
    last_message = state['messages'][-1]

    feedback_instructions = ""
    if manager_feedback:
        print(f"ğŸ“ ë§¤ë‹ˆì € í”¼ë“œë°± ìˆ˜ì‹  (Team 3): {manager_feedback}")
        feedback_instructions = f"""
**IMPORTANT REVISION INSTRUCTION FROM MANAGER:**
Your previous answer was not satisfactory. You MUST revise your answer based on the following feedback:
"{manager_feedback}"
"""
        state["manager_feedback"] = None

    if isinstance(last_message, ToolMessage) and last_message.name == "team3_evaluator" and last_message.content.startswith("retry"):
        internal_feedback = last_message.content.replace("retry:", "").strip()
        if internal_feedback:
            print(f"ğŸ“ íŒ€ ë‚´ë¶€ í”¼ë“œë°± ìˆ˜ì‹  (Team 3): {internal_feedback}")
            feedback_instructions += f"""
            **IMPORTANT INTERNAL FEEDBACK FOR REVISION:**
            Your previous answer failed the internal quality check. You MUST revise your answer based on the following issue:
            "{internal_feedback}"
            """

    context = _get_context_from_history(state)
    
    question = context["q_en_transformed"]
    output_format = context["output_format"]
    docs = context["docs"]
    out_type, answer_language = output_format[0], output_format[1]

    if docs:
        print("... ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±")
        prompt = DOCS_ANSWER_PROMPT  # DOCS_ANSWER_PROMPT ë³¸ë¬¸ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        # --- RAG/WEB ì»¨í…ìŠ¤íŠ¸ ë¶„ë¦¬ ---
        rag_ctx = format_docs(context.get("rag_docs", []) or [])
        web_ctx = format_docs(context.get("web_docs", []) or [])
        # í…œí”Œë¦¿ì´ ìš”êµ¬í•˜ëŠ” ë³€ìˆ˜ë§Œ ì•ˆì „í•˜ê²Œ ì£¼ì…
        input_vars = set(getattr(prompt, "input_variables", []))
        invoke_params = {
            "q_en_transformed": question,
            "out_type": out_type,
            "answer_language": answer_language,
        }
        if "rag_context" in input_vars:
            invoke_params["rag_context"] = rag_ctx
        if "web_context" in input_vars:
            invoke_params["web_context"] = web_ctx
    else:
        print("... LLM ìì²´ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±")
        prompt = GENERAL_ANSWER_PROMPT
        invoke_params = {
            "q_en_transformed": question,
            "out_type": out_type,
            "answer_language": answer_language,
        }

    llm = ChatOpenAI(model=config.LLM_MODEL_TEAM3_GEN, temperature=0.0)
    chain = prompt.partial(feedback_instructions=feedback_instructions) | llm

    try:
        result = chain.invoke(invoke_params)
        final_content = result.content.strip()

        if out_type == "table" and final_content.startswith("|"):
            try:
                # ìƒˆë¡œ ì¶”ê°€í•œ ë„êµ¬ë¥¼ ì§ì ‘ í˜¸ì¶œí•©ë‹ˆë‹¤.
                image_path = create_table_image.func(markdown_string=final_content)
                if not image_path.startswith("Error"):
                    # ì„±ê³µ ì‹œ, ë§ˆí¬ë‹¤ìš´ê³¼ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ í•¨ê»˜ í¬í•¨
                    final_content += f"\n\n---\n\n**[ìƒì„±ëœ í‘œ ì´ë¯¸ì§€ ë³´ê¸°]({image_path})**"
                else:
                    # ì‹¤íŒ¨ ì‹œ, ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë‹µë³€ì— í¬í•¨
                    final_content += f"\n\n(ì°¸ê³ : í‘œ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨ - {image_path})"
            except Exception as tool_e:
                print(f"âš ï¸ Table Image Tool ì§ì ‘ í˜¸ì¶œ ì‹¤íŒ¨: {tool_e}")
                final_content += f"\n\n(ì°¸ê³ : í‘œ ì´ë¯¸ì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ)"


        return {"messages": [AIMessage(content=final_content)]}
    except Exception as e:
        print(f"âŒ Team 3 (ë‹µë³€ ìƒì„±) ì˜¤ë¥˜: {e}")
        return {"messages": [ToolMessage(content=f"fail: Team3 Worker ì˜¤ë¥˜ - {e}", name="team3_worker", tool_call_id=str(uuid.uuid4()))]}

# --- Node 2: ë‹µë³€ í‰ê°€ (Evaluator) ---
def evaluate_answer(state: AgentState) -> Dict[str, Any]:
    print("--- AGENT: Team 3 (ë‹µë³€ í‰ê°€) ì‹¤í–‰ ---")
    generated_answer_msg = state['messages'][-1]
    if not isinstance(generated_answer_msg, AIMessage):
        return {"messages": [ToolMessage(content="fail: í‰ê°€í•  ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", name="team3_evaluator", tool_call_id=str(uuid.uuid4()))]}
    
    current_retries = state.get("team3_retries", 0)
    state["team3_retries"] = current_retries + 1
    
    answer = generated_answer_msg.content
    context = _get_context_from_history(state)
    question = context["q_en_transformed"]
    output_format = context["output_format"]
    
    if not all([question, output_format, answer]):
        return {"messages": [ToolMessage(content="fail: í‰ê°€ì— í•„ìš”í•œ ì •ë³´ ë¶€ì¡±", name="team3_evaluator", tool_call_id=str(uuid.uuid4()))]}
    
    parser = JsonOutputParser(p_object=AnswerEvaluationResult)
    prompt = PromptTemplate.from_template("""
You are the Team 3 Supervisor evaluator. Judge the final answer against the requested format,
the refined question, AND the provided documents.

Inputs:
[Refined question]
{q_en_transformed}

[Output format]  # ["type", "language"]
{output_format}

[Generated answer]
{generated_answer}

[Retrieved docs]
{retrieved_docs}

Scoring policy (deterministic):
- For EACH criterion, choose ONE value from {{0, 0.25, 0.50, 0.75, 1.00}}.
- Use the anchor descriptions below. If borderline, ROUND DOWN to the nearest anchor.
- Provide a concise error_message highlighting the single most limiting issue when ANY score < 0.75; otherwise use an empty string.
- Return JSON ONLY with the EXACT keys (no extra or missing): rules_compliance, question_coverage, hallucination_score, error_message.
- If any score would be N/A, still include the key with 0.00.
- If the [Generated answer] contains a Markdown table AND a link to an image file (e.g., `[ìƒì„±ëœ í‘œ ì´ë¯¸ì§€ ë³´ê¸°](...)`), you MUST IGNORE the image link part for your evaluation. Your assessment should be based solely on the content and structure of the Markdown table itself.

Criteria & anchors:

1) rules_compliance (float): Adherence to hard rules (requested type/language/structure; no guessing beyond docs; no URLs/academic-style citations; no think/reasoning reveal; correct subtle attribution if Web info is used).
   - 1.00: Fully matches requested type & language; respects all hard rules; attribution phrased correctly when needed and if Web Passages were used, the answer clearly signals web origin once (inline or closing note), phrased naturally without URLs/numeric citations.
   - 0.75: Minor format/tone slips but type/language correct; no safety/grounding violations.
   - 0.50: Multiple minor issues or one significant format error (e.g., partially wrong structure) but salvageable and Web Passages used but no explicit web attribution; or multiple redundant attributions.
   - 0.25: Major type/structure mismatch or language mix; partial rule breaches (e.g., informal citations) without safety breach.
   - 0.00: Ignores requested type/language or violates hard rules (e.g., reveals reasoning, adds URLs/academic citations, unsafe content) and Uses URLs/numeric citations/footnotes for web attribution; or mislabels the source.

2) question_coverage (float): Degree to which the answer addresses the refined question (intent, scope, constraints, sub-questions).
   - 1.00: Covers all core requirements, constraints, and sub-parts; anticipates edge cases as appropriate.
   - 0.75: Covers the main intent and most constraints; minor omissions that donâ€™t affect utility.
   - 0.50: Partial coverage; misses at least one key requirement or constraint.
   - 0.25: Largely off-target; touches the topic but not the userâ€™s actual need.
   - 0.00: Irrelevant or fails to address the question.

3) hallucination_score (float): Groundedness in retrieved docs (RAG/Web); correct application of â€œRAG takes precedenceâ€; proper subtle attribution when Web info is used.
   - 1.00: All claims directly supported; no contradictions; attribution used correctly when needed.
   - 0.75: Vast majority grounded; minor harmless inferences; no contradictions.
   - 0.50: Several claims weakly/implicitly supported or missing clear grounding.
   - 0.25: Many claims unsupported; suspected fabrication.
   - 0.00: Mostly conjecture or contradicts the documents.

Return JSON ONLY with:
{schema}
""").partial(schema=parser.get_format_instructions())
    llm = ChatOpenAI(
        model=config.LLM_MODEL_TEAM3_EVAL,
        temperature=0.0,
        model_kwargs={"response_format": {"type": "json_object"}}
    )
    chain = prompt | llm | parser

    try:
        result_dict = chain.invoke({
            "q_en_transformed": question,
            "output_format": json.dumps(output_format, ensure_ascii=False),
            "generated_answer": answer,
            "retrieved_docs": format_docs(context["docs"])
        })
        result = AnswerEvaluationResult.model_validate(result_dict)

        is_simple = state.get("is_simple_query", "No")

        if is_simple == "Yes":
            # ê°„ë‹¨ì§ˆë¬¸: retrieved docs ì—†ì–´ë„ í—ˆìš©
            passed = (result.rules_compliance and result.question_coverage >= 0.7)
        else:
            # ì¼ë°˜ì§ˆë¬¸: grounding í•„ìˆ˜
            passed = (result.rules_compliance and 
                    result.question_coverage >= 0.7 and 
                    result.hallucination_score >= 0.7)

        if passed:
            return {"messages": [ToolMessage(content="pass", name="team3_evaluator", tool_call_id=str(uuid.uuid4()))]}
        else:
            if current_retries < config.MAX_RETRIES_TEAM3:
                print(f"ğŸ” Team 3 í‰ê°€ ì‹¤íŒ¨. ì¬ì‹œë„ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤. ({current_retries + 1}/{config.MAX_RETRIES_TEAM3})")
                err = result.error_message or "ë‹µë³€ í’ˆì§ˆ ë¯¸ë‹¬ (Answer quality is insufficient)"
                return {"messages": [ToolMessage(content=f"retry: {err}", name="team3_evaluator", tool_call_id=str(uuid.uuid4()))]}
            else:
                print(f"âŒ Team 3 ìµœì¢… ì‹¤íŒ¨ (ì¬ì‹œë„ {config.MAX_RETRIES_TEAM3}íšŒ ì´ˆê³¼).")
                return {"messages": [ToolMessage(content="fail: ë‹µë³€ í’ˆì§ˆ ë¯¸ë‹¬", name="team3_evaluator", tool_call_id=str(uuid.uuid4()))]}

    except Exception as e:
        print(f"âŒ Team 3 (ë‹µë³€ í‰ê°€) ì˜¤ë¥˜: {e}")
        if current_retries < config.MAX_RETRIES_TEAM3:
            return {"messages": [ToolMessage(content="retry", name="team3_evaluator", tool_call_id=str(uuid.uuid4()))]}
        else:
            return {"messages": [ToolMessage(content=f"fail: Team3 Evaluator ì˜¤ë¥˜ - {e}", name="team3_evaluator", tool_call_id=str(uuid.uuid4()))]}
