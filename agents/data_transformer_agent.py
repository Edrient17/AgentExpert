# ë°ì´í„°ë¥¼ ë³€í™˜í•˜ê³  ë²¡í„° ì €ì¥ì†Œë¡œ ì €ì¥í•˜ëŠ” ì—ì´ì „íŠ¸

import os
from PIL import Image
import pytesseract

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document

# ì´ë¯¸ì§€ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  LangChain ë¬¸ì„œë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
def extract_text_from_image(image_path: str) -> Document:
    image = Image.open(image_path)
    text = pytesseract.image_to_string(image, lang='kor+eng')
    return Document(page_content=text, metadata={"source": image_path})

# ë©”ì¸ í•¨ìˆ˜: PDFì™€ ì´ë¯¸ì§€ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë²¡í„° ì €ì¥ì†Œë¡œ ë³€í™˜
def ingest_documents(
    data_path: str = "data/",
    output_path: str = "vector_store/",
    model_name="jhgan/ko-sbert-nli"
):

    print(f"ğŸ“‚ '{data_path}' ë‚´ PDF ë¡œë”© ì¤‘...")
    docs = []

    # ğŸ“„ PDF ì²˜ë¦¬
    pdf_files = [f for f in os.listdir(data_path) if f.endswith(".pdf")]
    if not pdf_files:
        print("ğŸš« PDF íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return {"message": "PDF ì—†ìŒ"}

    for filename in pdf_files:
        full_path = os.path.join(data_path, filename)
        try:
            loader = PyMuPDFLoader(full_path)
            loaded_docs = loader.load()
            docs.extend(loaded_docs)
            print(f"âœ… {filename}: {len(loaded_docs)}ê°œ ë¬¸ì„œ ë¡œë“œë¨")
        except Exception as e:
            print(f"âš ï¸ {filename} ë¡œë“œ ì‹¤íŒ¨: {e}")


    # ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬
    image_files = [f for f in os.listdir(data_path) if f.endswith((".jpg", ".png", ".jpeg"))]
    for filename in image_files:
        full_path = os.path.join(data_path, filename)
        try:
            doc = extract_text_from_image(full_path)
            docs.append(doc)
            print(f"âœ… ì´ë¯¸ì§€ {filename}: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ {filename} OCR ì‹¤íŒ¨: {e}")


    print(f"ì´ {len(docs)}ê°œ ë¬¸ì„œ ë¡œë“œë¨")

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    chunks = splitter.split_documents(docs)
    print(f"ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±ë¨")

    print(f"ì„ë² ë”© ëª¨ë¸: {model_name}")
    embedding = HuggingFaceEmbeddings(model_name=model_name)

    print(f"FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
    db = FAISS.from_documents(chunks, embedding)
    db.save_local(output_path)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")

    return {"message": f"{len(chunks)} chunks ì €ì¥ ì™„ë£Œ"}

# CLI ì‹¤í–‰ìš©
if __name__ == "__main__":
    ingest_documents()
