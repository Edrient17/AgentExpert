
# agents/team1_agents.py

import json
import uuid
from typing import List, Dict, Any

from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.messages import AIMessage, ToolMessage, HumanMessage
from pydantic import BaseModel, Field

import config
from state import AgentState

# --- Pydantic ìŠ¤í‚¤ë§ˆ (ë³€ê²½ ì—†ìŒ) ---
class QuestionProcessingResult(BaseModel):
    q_validity: bool = Field(description="ì‚¬ìš©ì ì§ˆë¬¸ì´ ë‹µë³€ ê°€ëŠ¥í•œ ìœ íš¨í•œ ì§ˆë¬¸ì¸ì§€ ì—¬ë¶€ (True/False)")
    q_en_transformed: str = Field(description="ì‚¬ìš©ì ì§ˆë¬¸ì„ ëª…í™•í•˜ê²Œ ì¬êµ¬ì„±í•œ ì˜ë¬¸ ì§ˆë¬¸")
    rag_queries: List[str] = Field(description="ê²€ìƒ‰ì— ì‚¬ìš©í•  2-4ê°œì˜ ë‹¤ì–‘í•œ ì˜ë¬¸ RAG ì¿¼ë¦¬ í›„ë³´ ë¦¬ìŠ¤íŠ¸", min_items=2, max_items=4)
    output_format: List[str] = Field(description="ìš”ì²­ëœ ì¶œë ¥ í¬ë§· [type, language]", min_items=2, max_items=2)

class QuestionEvaluationResult(BaseModel):
    semantic_alignment: float = Field(ge=0.0, le=1.0, description="ì‚¬ìš©ì ì…ë ¥ê³¼ q_en_transformedì˜ ì˜ë¯¸ì  ì •í•©ì„± ì ìˆ˜ [0,1]")
    format_compliance: bool
    rag_query_scores: List[float] = Field(default_factory=list)
    error_message: str = ""

# --- Node 1: ì§ˆë¬¸ ì²˜ë¦¬ (Worker) ---
def process_question(state: AgentState) -> Dict[str, Any]:
    print("--- AGENT: Team 1 (ì§ˆë¬¸ ì²˜ë¦¬) ì‹¤í–‰ ---")

    manager_feedback = state.get("manager_feedback")
    last_message = state['messages'][-1]

    feedback_instructions = ""
    if manager_feedback:
        print(f"ğŸ“ ë§¤ë‹ˆì € í”¼ë“œë°± ìˆ˜ì‹ : {manager_feedback}")
        feedback_instructions = f"""
        **IMPORTANT REVISION INSTRUCTION FROM MANAGER:**
        The previous attempt was not good enough. You MUST incorporate the following feedback into your new result:
        "{manager_feedback}"
        """
        state["manager_feedback"] = None

    # Check for internal team feedback from the evaluator
    if isinstance(last_message, ToolMessage) and last_message.name == "team1_evaluator" and last_message.content.startswith("retry"):
        internal_feedback = last_message.content.replace("retry:", "").strip()
        if internal_feedback:
            print(f"ğŸ“ íŒ€ ë‚´ë¶€ í”¼ë“œë°± ìˆ˜ì‹ : {internal_feedback}")
            feedback_instructions += f"""
            **IMPORTANT INTERNAL FEEDBACK FOR REVISION:**
            Your previous attempt failed the internal quality check. You MUST address the following issue:
            "{internal_feedback}"
            """
        
    user_input = next((msg.content for msg in reversed(state['messages']) if isinstance(msg, HumanMessage)), "")
    if not user_input.strip():
        return {"messages": [ToolMessage(content="fail: ì…ë ¥ëœ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.", name="team1_worker", tool_call_id=str(uuid.uuid4()))]}

    parser = JsonOutputParser(p_object=QuestionProcessingResult)
    prompt = PromptTemplate.from_template("""
You are the first-stage agent in a RAG pipeline.

TASKS
1) q_validity: Decide if the user input is a valid, answerable question (True/False).
   - false if too vague / missing constraints / unsafe.
2) q_en_transformed: Rewrite the question into clear English (preserve domain terms, numbers, units).
3) rag_queries: Generate 2-4 concise and keyword-focused search queries. Each query should extract the essential nouns and core intent from the user's input.
   - Mix styles (keyword, semantic paraphrase, entity-focused, time-bounded) when applicable.
   - Do NOT invent facts not implied by the user input. Return 2â€“4 items only.
4) output_format: ALWAYS return a 2-item array [type, language].
   - type âˆˆ ["report","table","bulleted","qa"]
   - language âˆˆ ["ko","en"]
   - Defaults apply independently:
       â€¢ If type is missing/unclear/invalid â†’ use "qa".
       â€¢ If language is missing/unclear/invalid â†’ use "ko".
   - If only one of (type, language) can be inferred, fill the other with its default.
   - Normalize to lowercase. Return exactly two items, no more, no less.

{feedback_instructions}
                                          
STRICT OUTPUT (JSON ONLY, no prose):
{schema}

USER INPUT:
{user_input}
""").partial(schema=parser.get_format_instructions())
    llm = ChatOpenAI(
        model=config.LLM_MODEL_TEAM1,
        temperature=0.0,
        model_kwargs={"response_format": {"type": "json_object"}}
    )
    chain = prompt.partial(feedback_instructions=feedback_instructions, schema=parser.get_format_instructions()) | llm | parser

    try:
        result_dict = chain.invoke({"user_input": user_input})
        if not result_dict.get("rag_queries"):
            raise ValueError("rag_queriesê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return {
            "user_input": user_input,
            "q_en_transformed": result_dict.get("q_en_transformed", ""),
            "messages": [
                AIMessage(
                    content="ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.",
                    additional_kwargs=result_dict
                )
            ]
        }
    except Exception as e:
        print(f"âŒ Team 1 (ì§ˆë¬¸ ì²˜ë¦¬) ì˜¤ë¥˜: {e}")
        return {"messages": [ToolMessage(content=f"fail: Team1 Worker ì˜¤ë¥˜ - {e}", name="team1_worker", tool_call_id=str(uuid.uuid4()))]}

# --- Node 2: ì§ˆë¬¸ í‰ê°€ (Evaluator) ---
def evaluate_question(state: AgentState) -> Dict[str, Any]:
    print("--- AGENT: Team 1 (ê²°ê³¼ í‰ê°€) ì‹¤í–‰ ---")
    last_message = state['messages'][-1]
    if not isinstance(last_message, AIMessage) or not last_message.additional_kwargs:
        return {"messages": [ToolMessage(content="fail: Team1 í‰ê°€ìê°€ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", name="team1_evaluator", tool_call_id=str(uuid.uuid4()))]}

    current_retries = state.get("team1_retries", 0)
    state["team1_retries"] = current_retries + 1

    processed_data = last_message.additional_kwargs
    user_input = next((msg.content for msg in state['messages'] if isinstance(msg, HumanMessage)), "")

    q_validity = processed_data.get("q_validity", False)
    q_en_transformed = processed_data.get("q_en_transformed", "")
    rag_queries = processed_data.get("rag_queries", [])
    output_format = processed_data.get("output_format", ["qa", "ko"])
    
    if not q_validity or not all([user_input, q_en_transformed, rag_queries]):
        return {"messages": [ToolMessage(content="fail: í‰ê°€ì— í•„ìš”í•œ ì •ë³´ ë¶€ì¡±", name="team1_evaluator", tool_call_id=str(uuid.uuid4()))]}
    
    parser = JsonOutputParser(p_object=QuestionEvaluationResult)
    prompt = PromptTemplate.from_template("""
You are the Team1 Supervisor evaluator. Using the information below, make binary judgments and per-query scores.

[User Input]
{user_input}

[q_en_transformed]  # refined English question from the agent
{q_en_transformed}

[output_format]  # [type, language]
{output_format}

[default_format]
{default_format}

[rag_queries]
{rag_queries_json}

Criteria:
1) semantic_alignment (float in [0,1]): A continuous score for how accurately q_en_transformed reflects the meaning and constraints of user_input.
   - 1.0 = perfectly faithful; 0.0 = unrelated/incorrect.
2) format_compliance (bool): Follow these steps IN ORDER to decide:
   a) First, analyze [User Input]. Does it explicitly request a specific output format or language (e.g., "í‘œë¡œ", "ì˜ì–´ë¡œ", "in a table", "in English")?
   b) **If the user SPECIFIED a format:** `format_compliance` is TRUE if [output_format] correctly matches the user's request. **The [default_format] is IRRELEVANT and should be ignored in this case.**
   c) **If the user did NOT specify a format:** `format_compliance` is TRUE only if [output_format] is exactly the same as [default_format].
3) rag_query_scores (list[float]): For each rag_query, output a score in [0, 1] indicating how well it captures the userâ€™s requirements
   (entities/keywords, constraints, time ranges, numbers/units, search-friendliness). Length MUST equal len(rag_queries).
4) error_message (str): If anything is wrong or inconsistent, write a short Korean message describing the issue; otherwise return an empty string "".

Return JSON ONLY. Do not include any additional text.

Output schema:
{schema}
""").partial(schema=parser.get_format_instructions()) # í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì€ ê¸°ì¡´ê³¼ ë™ì¼
    llm = ChatOpenAI(
        model=config.LLM_MODEL_TEAM1,
        temperature=0.0,
        model_kwargs={"response_format": {"type": "json_object"}}
    )
    chain = prompt | llm | parser

    try:
        result_dict = chain.invoke({
            "user_input": user_input,
            "q_en_transformed": q_en_transformed,
            "output_format": json.dumps(output_format, ensure_ascii=False),
            "default_format": json.dumps(["qa", "ko"], ensure_ascii=False),
            "rag_queries_json": json.dumps(rag_queries, ensure_ascii=False)
        })
        result = QuestionEvaluationResult.model_validate(result_dict)

        if len(result.rag_query_scores) != len(rag_queries):
            raise ValueError("ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ì™€ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´ê°€ ë‹¤ë¦…ë‹ˆë‹¤.")
        
        passed = (result.semantic_alignment >= 0.8) and result.format_compliance
        if passed:
            best_idx = max(range(len(result.rag_query_scores)), key=lambda i: result.rag_query_scores[i])
            best_query = rag_queries[best_idx]
            
            return {
                # â¬‡ï¸ ìƒíƒœì— ì§ì ‘ ì €ì¥
                "best_rag_query": best_query,
                "q_en_transformed": q_en_transformed,
                "messages": [
                    ToolMessage(
                        content="pass",
                        name="team1_evaluator",
                        tool_call_id=str(uuid.uuid4()),
                        additional_kwargs={
                            "q_en_transformed": q_en_transformed,
                            "output_format": output_format,
                            "best_rag_query": best_query,
                        }
                    )
                ]
            }
        else:
            err = result.error_message or "Team1: í‰ê°€ ê¸°ì¤€ ë¯¸ë‹¬ (Team1: Evaluation criteria not met)"
            if current_retries < config.MAX_RETRIES_TEAM1:
                print(f"ğŸ” Team 1 í‰ê°€ ì‹¤íŒ¨. ì¬ì‹œë„ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤. ({current_retries + 1}/{config.MAX_RETRIES_TEAM1})")
                return {"messages": [ToolMessage(content=f"retry: {err}", name="team1_evaluator", tool_call_id=str(uuid.uuid4()))]}
            else:
                print(f"âŒ Team 1 ìµœì¢… ì‹¤íŒ¨ (ì¬ì‹œë„ {config.MAX_RETRIES_TEAM1}íšŒ ì´ˆê³¼).")
                return {"messages": [ToolMessage(content=f"fail: {err}", name="team1_evaluator", tool_call_id=str(uuid.uuid4()))]}
             
    except Exception as e:
        print(f"âŒ Team 1 (ê²°ê³¼ í‰ê°€) ì˜¤ë¥˜: {e}")
        if current_retries < config.MAX_RETRIES_TEAM1:
             return {"messages": [ToolMessage(content="retry", name="team1_evaluator", tool_call_id=str(uuid.uuid4()))]}
        else:
             return {"messages": [ToolMessage(content=f"fail: Team1 Evaluator ì˜¤ë¥˜ - {e}", name="team1_evaluator", tool_call_id=str(uuid.uuid4()))]}
