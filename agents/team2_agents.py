# agents/team2_agents.py

import json
import uuid
from typing import List, Dict, Any

from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.messages import ToolMessage
from pydantic import BaseModel, Field

import config
from state import AgentState
from utility_tools import vector_store_rag_search, serpapi_web_search, format_docs

# --- Pydantic ìŠ¤í‚¤ë§ˆ ---
class DocEvaluationResult(BaseModel):
    semantic_relevance: float = Field(ge=0.0, le=1.0, description="ë¬¸ì„œê°€ ì§ˆë¬¸ ì˜ë„ì™€ ì œì•½ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ [0,1]")
    is_detailed: float = Field(ge=0.0, le=1.0, description="ë¬¸ì„œê°€ ì¶©ë¶„íˆ êµ¬ì²´ì ì´ê³  ì„¸ë¶€ì ì¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì ìˆ˜ [0,1]")
    error_message: str = ""

def _get_query_from_history(state: AgentState) -> str:
    """ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì—ì„œ team1_evaluatorê°€ ì „ë‹¬í•œ best_rag_queryë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    for msg in reversed(state['messages']):
        if isinstance(msg, ToolMessage) and msg.name == "team1_evaluator":
            return msg.additional_kwargs.get("best_rag_query", "")
    return ""

# --- Node 1 & 2: RAG ë° ì›¹ ê²€ìƒ‰ ---
def rag_search(state: AgentState) -> Dict[str, Any]:
    print("--- AGENT: Team 2 (RAG ê²€ìƒ‰) ì‹¤í–‰ ---")

    rag_query = _get_query_from_history(state)
    if not rag_query:
        return {"messages": [ToolMessage(content="fail: RAG ì¿¼ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", name="rag_search", tool_call_id=str(uuid.uuid4()))]}
        
    try:
        rag_docs = vector_store_rag_search.func(rag_query)
        return {
            "messages": [
                ToolMessage(
                    content=format_docs(rag_docs),
                    name="rag_search_result",
                    tool_call_id=str(uuid.uuid4()), 
                    additional_kwargs={"source_docs": rag_docs}
                )
            ]
        }
    except Exception as e:
        print(f"âŒ Team 2 (RAG ê²€ìƒ‰) ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return {"messages": [ToolMessage(content=f"fail: RAG ê²€ìƒ‰ ì˜¤ë¥˜ - {e}", name="rag_search", tool_call_id=str(uuid.uuid4()))]}

def web_search(state: AgentState) -> Dict[str, Any]:
    print("--- AGENT: Team 2 (ì›¹ ê²€ìƒ‰) ì‹¤í–‰ ---")
    rag_query = _get_query_from_history(state)
    try:
        web_docs = serpapi_web_search.func(rag_query)
        return {
            "messages": [
                ToolMessage(
                    content=format_docs(web_docs),
                    name="web_search_result",
                    tool_call_id=str(uuid.uuid4()), 
                    additional_kwargs={"source_docs": web_docs}
                )
            ]
        }
    except Exception as e:
        print(f"âŒ Team 2 (ì›¹ ê²€ìƒ‰) ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return {"messages": [ToolMessage(content=f"fail: ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜ - {e}", name="web_search", tool_call_id=str(uuid.uuid4()))]}

# --- Node 3: ë¬¸ì„œ í‰ê°€ (Evaluator) ---
def evaluate_documents(state: AgentState) -> Dict[str, Any]:
    print("--- AGENT: Team 2 (ë¬¸ì„œ í‰ê°€) ì‹¤í–‰ ---")
    last_message = state['messages'][-1]
    
    docs_to_evaluate = last_message.additional_kwargs.get("source_docs", [])
    search_source = "web_search" if last_message.name == "web_search_result" else "rag_search"

    current_retries = state.get("team2_retries", 0)

    if not docs_to_evaluate:
        passed = False
        error_msg = "í‰ê°€í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
    else:
        q_en_transformed, rag_query = "", ""
        for msg in reversed(state['messages']):
            if isinstance(msg, ToolMessage) and msg.name == "team1_evaluator":
                q_en_transformed = msg.additional_kwargs.get("q_en_transformed", "")
                rag_query = msg.additional_kwargs.get("best_rag_query", "")
                break

        parser = JsonOutputParser(p_object=DocEvaluationResult)
        prompt = PromptTemplate.from_template("""
You are the Team2 Supervisor evaluator. Given the question summary and retrieved docs,
decide whether the docs are good enough to support answering the question.

[Question Summary]
{q_en_transformed}

[RAG Query]
{rag_query}

[Retrieved Docs Preview]  # concatenated & possibly truncated
{docs_preview}

Return JSON ONLY with the following fields:
- semantic_relevance (float in [0,1]): Do the docs match the user's intent and constraints?
- is_detailed (float in [0,1]): Do the docs collectively contain enough specifics to answer the question reliably?
- error_message (str): If anything is wrong (empty/irrelevant/too generic/duplicated), write a short Korean message; else "".

Output schema:
{schema}
""").partial(schema=parser.get_format_instructions())
        llm = ChatOpenAI(
            model=config.LLM_MODEL_TEAM2_EVAL,
            temperature=0.0,
            model_kwargs={"response_format": {"type": "json_object"}}
        )
        chain = prompt | llm | parser

        try:
            result_dict = chain.invoke({
                "q_en_transformed": q_en_transformed,
                "rag_query": rag_query,
                "docs_preview": format_docs(docs_to_evaluate)
            })
            result = DocEvaluationResult.model_validate(result_dict)
            passed = (result.semantic_relevance >= 0.7) and (result.is_detailed >= 0.7)
            error_msg = result.error_message or "Team2: ë¬¸ì„œ í’ˆì§ˆ ë¯¸ë‹¬"
        except Exception as e:
            print(f"âŒ Team 2 (ë¬¸ì„œ í‰ê°€) LLM ì˜¤ë¥˜: {e}")
            passed = False
            error_msg = f"Team2 Evaluator LLM ì˜¤ë¥˜ - {e}"

    if passed:
        return {
            "messages": [
                ToolMessage(
                    content="pass", 
                    name="team2_evaluator",
                    tool_call_id=str(uuid.uuid4()),
                    additional_kwargs={
                        "source": search_source,
                        "retrieved_docs": docs_to_evaluate
                    }
                )
            ],
            "team2_retries": current_retries + 1
        }
    else: # í‰ê°€ ì‹¤íŒ¨ ì‹œ
        if current_retries == 0:
            print(f"ğŸ” RAG í‰ê°€ ì‹¤íŒ¨. ì¬ì‹œë„ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤. ({current_retries + 1}/{config.MAX_RETRIES_TEAM2})")
            return {
                "messages": [ToolMessage(content="retry_rag", name="team2_evaluator", tool_call_id=str(uuid.uuid4()))],
                "team2_retries": current_retries + 1
            }
        elif current_retries == 1:
            print(f"ğŸ” RAG ìµœì¢… ì‹¤íŒ¨. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.")
            return {
                "messages": [ToolMessage(content="fallback_to_web", name="team2_evaluator", tool_call_id=str(uuid.uuid4()))],
                "team2_retries": current_retries + 1
            }
        elif current_retries < config.MAX_RETRIES_TEAM2:
            print(f"ğŸ” WEB í‰ê°€ ì‹¤íŒ¨. ì¬ì‹œë„ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤. ({current_retries + 1}/{config.MAX_RETRIES_TEAM2})")
            return {
                "messages": [ToolMessage(content="retry_web", name="team2_evaluator", tool_call_id=str(uuid.uuid4()))],
                "team2_retries": current_retries + 1
            }
        else:
            print(f"âŒ WEB ìµœì¢… ì‹¤íŒ¨.")
            return {
                "messages": [ToolMessage(content=f"fail: {error_msg}", name="team2_evaluator", tool_call_id=str(uuid.uuid4()))],
                "team2_retries": current_retries + 1
            }