import json
from langchain_core.messages import HumanMessage

from agents.team1_agents import process_question, QuestionEvaluationResult
from agents.team2_agents import DocEvaluationResult
from agents.team3_agents import generate_answer
from utility_tools import vector_store_rag_search, format_docs
import config

from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel

class AnswerEvaluationResult(BaseModel):
    rules_compliance: bool
    question_coverage: float
    hallucination_score: float
    error_message: str = ""

def run_naive_rag(user_question: str):
    """
    ëª¨ë“  ë‹¨ê³„ì˜ í‰ê°€ë¥¼ í¬í•¨í•˜ëŠ” ë‹¨ìˆœí™”ëœ ìˆœì°¨ì  RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    print(f"ğŸš€ Naive RAG íŒŒì´í”„ë¼ì¸ (ì „ì²´ í‰ê°€ í¬í•¨)ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì§ˆë¬¸: '{user_question}'")

    # --- 1. ì´ˆê¸° ìƒíƒœ ì„¤ì • ---
    state = {
        "messages": [HumanMessage(content=user_question)],
        "rag_docs": [],
        "web_docs": []
    }

    # --- 2. Team1 Worker: ì§ˆë¬¸ ì²˜ë¦¬ ---
    print("\n--- 1. ì§ˆë¬¸ ì²˜ë¦¬ (Team 1) ---")
    t1_result = process_question(state)
    t1_message = t1_result['messages'][0]
    state['messages'].append(t1_message)

    processed_data = t1_message.additional_kwargs
    q_en_transformed = processed_data.get("q_en_transformed", "")
    rag_queries = processed_data.get("rag_queries", [])
    output_format = processed_data.get("output_format", ["qa", "ko"])

    if not rag_queries:
        print("âŒ Team 1ì—ì„œ RAG ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    first_rag_query = rag_queries[0]
    state['q_en_transformed'] = q_en_transformed
    state['output_format'] = output_format
    state['best_rag_query'] = first_rag_query

    print(f"âœ… ì²˜ë¦¬ëœ ì§ˆë¬¸: {q_en_transformed}")
    print(f"âœ… ì‚¬ìš©í•  RAG ì¿¼ë¦¬: '{first_rag_query}'")

    # --- 3. Team1 Evaluator: ì§ˆë¬¸ ì²˜ë¦¬ ê²°ê³¼ í‰ê°€ (ì ìˆ˜ ì¸¡ì • ì „ìš©) ---
    print("\n--- 2. ì§ˆë¬¸ ì²˜ë¦¬ í‰ê°€ (Team 1) ---")
    t1_parser = JsonOutputParser(p_object=QuestionEvaluationResult)
    t1_prompt = PromptTemplate.from_template("""
You are the Team1 Supervisor evaluator. Using the information below, make binary judgments and per-query scores.

[User Input]
{user_input}

[q_en_transformed]  # refined English question from the agent
{q_en_transformed}

[output_format]  # [type, language]
{output_format}

[default_format]
{default_format}

[rag_queries]
{rag_queries_json}

[Scoring Guide]

You must evaluate the document using the following criteria.
You must select only one of the predefined values. Do not output any intermediate values.

1) semantic_alignment (float in [0,1])  
   - Definition: How accurately `q_en_transformed` reflects the meaning and constraints of `user_input`.  
   - Allowed values: {{0.00, 0.25, 0.50, 0.75, 1.00}}  
     * 0.00 (None): completely irrelevant or incorrect  
     * 0.25 (Low): only some keywords overlap, misses the core meaning/constraints  
     * 0.50 (Partial): captures main meaning but lacks important constraints or nuance  
     * 0.75 (Strong): satisfies most meaning/constraints with solid coverage, but small gaps remain  
     * 1.00 (Exact): fully faithful to meaning/constraints, immediately usable  

2) format_compliance (bool)  
   - Definition: Whether the `output_format` matches the userâ€™s requested format.  
   - Decision process:  
     a) Analyze `user_input`. Did the user explicitly request a format/language (e.g., "í‘œë¡œ", "ì˜ì–´ë¡œ", "in a table", "in English")?  
     b) If YES â†’ `format_compliance = true` only if `output_format` exactly matches the userâ€™s request (ignore default_format).  
     c) If NO â†’ `format_compliance = true` only if `output_format` == `default_format`.  

3) rag_query_scores (list[float])  
   - Definition: For each `rag_query`, score how well it captures the userâ€™s requirements (keywords, constraints, time ranges, numbers/units, search-friendliness).  
   - Each value must be in [0.00, 1.00], continuous scale.  
   - The length MUST equal len(rag_queries).  

4) error_message (str)  
   - If the document is empty, irrelevant, too generic, or duplicated â†’ return a short note in Korean.  
   - Otherwise return "" (empty string).
                                          
Return JSON ONLY. Do not include any additional text.

Output schema:
{schema}
""").partial(schema=t1_parser.get_format_instructions())
    t1_llm = ChatOpenAI(model=config.LLM_MODEL_TEAM1, temperature=0.0, model_kwargs={"response_format": {"type": "json_object"}})
    t1_chain = t1_prompt | t1_llm | t1_parser
    try:
        t1_eval_dict = t1_chain.invoke({
            "user_input": user_question,
            "q_en_transformed": q_en_transformed,
            "output_format": json.dumps(output_format, ensure_ascii=False),
            "default_format": json.dumps(["qa", "ko"], ensure_ascii=False),
            "rag_queries_json": json.dumps(rag_queries, ensure_ascii=False)
        })
        print("âœ… Team 1 í‰ê°€ ì™„ë£Œ:")
        print(json.dumps(t1_eval_dict, indent=2, ensure_ascii=False))
    except Exception as e:
        print(f"âŒ Team 1 í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # --- 4. Team2 Worker: ë¬¸ì„œ ê²€ìƒ‰ (ë„êµ¬ë¥¼ ì§ì ‘ í˜¸ì¶œ) ---
    print("\n--- 3. ë¬¸ì„œ ê²€ìƒ‰ (Team 2) ---")
    try:
        rag_docs = vector_store_rag_search.func(query=first_rag_query, top_k=5, rerank_k=5)
        state['rag_docs'] = rag_docs
        print(f"âœ… {len(rag_docs)}ê°œì˜ ë¬¸ì„œë¥¼ RAG ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # --- 5. Team2 Evaluator: ê²€ìƒ‰ ë¬¸ì„œ í‰ê°€ (ì ìˆ˜ ì¸¡ì • ì „ìš©) ---
    print("\n--- 4. ê²€ìƒ‰ ë¬¸ì„œ í‰ê°€ (Team 2) ---")
    t2_parser = JsonOutputParser(p_object=DocEvaluationResult)
    t2_prompt = PromptTemplate.from_template("""
You are a strict Quality Control Supervisor evaluator.
Your job is to carefully assess whether the given document is sufficiently relevant and detailed to help answer the userâ€™s question.
Follow the instructions below without deviation and return ONLY a valid JSON object matching the schema.

[Evaluation Guidelines]
- Use the two scoring rubrics below independently: one for semantic_relevance and one for is_detailed.
- Judge only based on the provided inputs. Do not invent information.
- All contents of the document should be considered, not just part of the document.

[Scoring Guide â€” semantic_relevance]
Choose EXACTLY one level for how well the document matches the questionâ€™s intent and constraints (subject, entities, context).
- 0.00 = None: completely irrelevant or empty; contradicts the question or ignores core entities/constraints.
- 0.25 = Low: superficial keyword overlap; misses main intent or key constraints; noticeable topic drift.
- 0.50 = Partial: addresses the main topic but misses important constraints/context; mixed or uneven relevance.
- 0.75 = Strong: satisfies most intent/constraints with minor mismatches or small gaps.
- 1.00 = Exact: fully aligned with the questionâ€™s entities and constraints; no topic drift.

[Scoring Guide â€” is_detailed]
Choose EXACTLY one level for how specific and sufficient the document is to support a reliable answer.
- 0.00 = None: empty/generic; no actionable specifics.
- 0.25 = Low: few specifics; vague statements; lacks steps, data, or concrete facts.
- 0.50 = Partial: some specifics but missing key details to answer fully; incomplete coverage.
- 0.75 = Strong: solid specifics; covers most needed details with minor gaps.
- 1.00 = Exact: comprehensive and specific (e.g., steps, parameters, examples, citations, or numbers); fully sufficient.

[Error Message]
- If the document is empty, irrelevant, duplicated, off-topic, or too generic/outdated for the question, write a short English note (<= 20 words).
- Otherwise, return "".

[Inputs]
- Question Summary: {q_en_transformed}
- RAG Query: {rag_query}
- Document (excerpted for evaluation): {doc_text}

[Output Instructions]
- Return ONLY a valid JSON object; no commentary, Markdown, code fences, or extra keys.
- Keys must exactly match the schema fields.
- Values for the two scores MUST be one of: 0.00, 0.25, 0.50, 0.75, 1.00.

Output schema:
{schema}
""").partial(schema=t2_parser.get_format_instructions())
    t2_llm = ChatOpenAI(model=config.LLM_MODEL_TEAM2_EVAL, temperature=0.0, model_kwargs={"response_format": {"type": "json_object"}})
    t2_chain = t2_prompt | t2_llm | t2_parser

    for i, doc in enumerate(rag_docs):
        try:
            preview = (getattr(doc, "page_content", "") or "")[:4000]
            t2_eval_dict = t2_chain.invoke({"q_en_transformed": q_en_transformed, "rag_query": first_rag_query, "doc_text": preview})
            print(f"  - ë¬¸ì„œ #{i+1} í‰ê°€ ì ìˆ˜: {json.dumps(t2_eval_dict, ensure_ascii=False)}")
        except Exception as e:
            print(f"  - ë¬¸ì„œ #{i+1} í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("âœ… ëª¨ë“  ê²€ìƒ‰ ë¬¸ì„œì— ëŒ€í•œ ê°œë³„ í‰ê°€ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")

    # --- 6. Team3 Worker: ë‹µë³€ ìƒì„± ---
    print("\n--- 5. ë‹µë³€ ìƒì„± (Team 3) ---")
    t3_gen_result = generate_answer(state)
    final_answer_msg = t3_gen_result['messages'][0]
    state['messages'].append(final_answer_msg)

    print("âœ… ìµœì¢… ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("="*30)
    print(final_answer_msg.content)
    print("="*30)

    # --- 7. Team3 Evaluator: ìµœì¢… ë‹µë³€ í‰ê°€ (ì ìˆ˜ ì¸¡ì • ì „ìš©) ---
    print("\n--- 6. ìµœì¢… ë‹µë³€ í‰ê°€ (Team 3) ---")
    t3_parser = JsonOutputParser(p_object=AnswerEvaluationResult)
    t3_prompt = PromptTemplate.from_template("""
You are the Team 3 Supervisor evaluator. Judge the final answer against the requested format,
the refined question, AND the provided documents.

Inputs:
[Refined question]
{q_en_transformed}

[Output format]  # ["type", "language"]
{output_format}

[Generated answer]
{generated_answer}

[Retrieved docs]
{retrieved_docs}

Scoring policy (deterministic):
- For EACH criterion, choose ONE value from {{0, 0.25, 0.50, 0.75, 1.00}}.
- Use the anchor descriptions below. If borderline, ROUND DOWN to the nearest anchor.
- Provide a concise error_message highlighting the single most limiting issue when ANY score < 0.75; otherwise use an empty string.
- Return JSON ONLY with the EXACT keys (no extra or missing): rules_compliance, question_coverage, hallucination_score, error_message.
- If any score would be N/A, still include the key with 0.00.
- If the [Generated answer] contains a Markdown table AND a link to an image file (e.g., `[ìƒì„±ëœ í‘œ ì´ë¯¸ì§€ ë³´ê¸°](...)`), you MUST IGNORE the image link part for your evaluation. Your assessment should be based solely on the content and structure of the Markdown table itself.

Criteria & anchors:

1) rules_compliance (float): Adherence to hard rules (requested type/language/structure; no guessing beyond docs; no URLs/academic-style citations; no think/reasoning reveal; correct subtle attribution if Web info is used).
   - 1.00: Fully matches requested type & language; respects all hard rules; attribution phrased correctly when needed and if Web Passages were used, the answer clearly signals web origin once (inline or closing note), phrased naturally without URLs/numeric citations.
   - 0.75: Minor format/tone slips but type/language correct; no safety/grounding violations.
   - 0.50: Multiple minor issues or one significant format error (e.g., partially wrong structure) but salvageable and Web Passages used but no explicit web attribution; or multiple redundant attributions.
   - 0.25: Major type/structure mismatch or language mix; partial rule breaches (e.g., informal citations) without safety breach.
   - 0.00: Ignores requested type/language or violates hard rules (e.g., reveals reasoning, adds URLs/academic citations, unsafe content) and Uses URLs/numeric citations/footnotes for web attribution; or mislabels the source.

2) question_coverage (float): Degree to which the answer addresses the refined question (intent, scope, constraints, sub-questions).
   - 1.00: Covers all core requirements, constraints, and sub-parts; anticipates edge cases as appropriate.
   - 0.75: Covers the main intent and most constraints; minor omissions that donâ€™t affect utility.
   - 0.50: Partial coverage; misses at least one key requirement or constraint.
   - 0.25: Largely off-target; touches the topic but not the userâ€™s actual need.
   - 0.00: Irrelevant or fails to address the question.

3) hallucination_score (float): Groundedness in retrieved docs (RAG/Web); correct application of â€œRAG takes precedenceâ€; proper subtle attribution when Web info is used.
   - 1.00: All claims directly supported; no contradictions; attribution used correctly when needed.
   - 0.75: Vast majority grounded; minor harmless inferences; no contradictions.
   - 0.50: Several claims weakly/implicitly supported or missing clear grounding.
   - 0.25: Many claims unsupported; suspected fabrication.
   - 0.00: Mostly conjecture or contradicts the documents.

Return JSON ONLY with:
{schema}
""").partial(schema=t3_parser.get_format_instructions())
    t3_llm = ChatOpenAI(model=config.LLM_MODEL_TEAM3_EVAL, temperature=0.0, model_kwargs={"response_format": {"type": "json_object"}})
    t3_chain = t3_prompt | t3_llm | t3_parser

    try:
        t3_eval_dict = t3_chain.invoke({
            "q_en_transformed": q_en_transformed,
            "output_format": json.dumps(output_format, ensure_ascii=False),
            "generated_answer": final_answer_msg.content,
            "retrieved_docs": format_docs(state["rag_docs"])
        })
        print("âœ… ìµœì¢… ë‹µë³€ í‰ê°€ ì™„ë£Œ:")
        print(json.dumps(t3_eval_dict, indent=2, ensure_ascii=False))
    except Exception as e:
        print(f"âŒ ë‹µë³€ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

#####################################################################################
if __name__ == "__main__":
    sample_question = "ê°¤ëŸ­ì‹œ S24ì˜ ì£¼ìš” ì¹´ë©”ë¼ ê¸°ëŠ¥ë“¤ì„ ì •ë¦¬í•´ì¤˜." # ì§ˆë¬¸ì„ ì—¬ê¸°ì— ì…ë ¥í•´ì£¼ì„¸ìš”!!!!!!
    run_naive_rag(sample_question)