# ingest_data.py

import os
from PIL import Image
import pytesseract

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document

# config íŒŒì¼ì—ì„œ ëª¨ë¸ ì´ë¦„ê³¼ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
import config

# Tesseract ì‹¤í–‰ íŒŒì¼ ê²½ë¡œ ì„¤ì • (Windows ì‚¬ìš©ìì˜ ê²½ìš° í•„ìš”í•  ìˆ˜ ìˆìŒ)
# ì˜ˆ: pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

def extract_text_from_image(image_path: str) -> Document:
    """ì´ë¯¸ì§€ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  LangChain ë¬¸ì„œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        image = Image.open(image_path)
        # í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ëª¨ë‘ ì¸ì‹í•˜ë„ë¡ ì„¤ì •
        text = pytesseract.image_to_string(image, lang='kor+eng')
        return Document(page_content=text, metadata={"source": os.path.basename(image_path)})
    except Exception as e:
        print(f"âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜ {image_path}: {e}")
        return None

def create_vector_store(
    data_path: str = "data/",
    vector_store_path: str = config.VECTOR_STORE_PATH,
    embedding_model: str = config.EMBEDDING_MODEL
):
    """
    ì§€ì •ëœ ê²½ë¡œì˜ ë¬¸ì„œ(PDF, ì´ë¯¸ì§€)ë¥¼ ë¡œë“œí•˜ì—¬ FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not os.path.exists(data_path):
        print(f"âŒ ë°ì´í„° í´ë” '{data_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í´ë”ë¥¼ ìƒì„±í•˜ê³  íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        return

    all_docs = []
    
    # 1. PDF ë° ì´ë¯¸ì§€ íŒŒì¼ ë¡œë“œ
    print(f"ğŸ“‚ '{data_path}' í´ë”ì—ì„œ ë¬¸ì„œ ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    for filename in os.listdir(data_path):
        full_path = os.path.join(data_path, filename)
        if filename.lower().endswith(".pdf"):
            try:
                loader = PyMuPDFLoader(full_path)
                docs = loader.load()
                all_docs.extend(docs)
                print(f"  ğŸ“„ PDF ë¡œë“œ ì™„ë£Œ: {filename} ({len(docs)} í˜ì´ì§€)")
            except Exception as e:
                print(f"  âš ï¸ PDF ë¡œë“œ ì‹¤íŒ¨: {filename} ({e})")
        elif filename.lower().endswith((".jpg", ".png", ".jpeg")):
            doc = extract_text_from_image(full_path)
            if doc:
                all_docs.append(doc)
                print(f"  ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë£Œ: {filename}")

    if not all_docs:
        print("ğŸš« ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. 'data' í´ë”ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # 2. ë¬¸ì„œ ë¶„í• 
    print(f"\nğŸŒ€ ì´ {len(all_docs)}ê°œì˜ ë¬¸ì„œë¥¼ í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_documents(all_docs)
    print(f"  âœ… ì´ {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ.")

    # 3. ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    print(f"\nğŸ§  ì„ë² ë”© ëª¨ë¸ '{embedding_model}'ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    embedding = HuggingFaceEmbeddings(model_name=embedding_model)
    
    db = FAISS.from_documents(chunks, embedding)
    db.save_local(vector_store_path)
    
    print(f"\nâœ¨ FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"  ğŸ“ ì €ì¥ ê²½ë¡œ: '{vector_store_path}'")


if __name__ == "__main__":
    # ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ë©´ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    create_vector_store()
